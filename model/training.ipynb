{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from os import walk\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# dataset is here: smb://nas-weber01.unisg.ch/data/Nassy/03_Online_Model/features\n",
    "_, _, file_names = next(walk('./dataset'))\n",
    "\n",
    "files = [(name.split('_')[0], name.split('_')[1].split('.')[0], name) for name in file_names]\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for (subject_number, task, file_name) in files:\n",
    "    part = pd.read_csv(f'./dataset/{file_name}')\n",
    "    labels = pd.read_csv(f'./labels/{subject_number}.csv', sep=\";\").set_index('task')\n",
    "    rating = float(labels.loc[task]['r'])\n",
    "    part['label'] = int(rating > 4)\n",
    "    part['subject'] = subject_number\n",
    "\n",
    "    dataset = pd.concat([part, dataset])\n",
    "    \n",
    "dataset.tail(80)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   subject task    d_mean  d_median     d_std     e_min     e_max    e_mean  \\\n",
       "0      003   t2  0.220919  0.259452 -0.088826  2.173757  3.732305  2.909432   \n",
       "0      001   t8  0.644607  0.904941 -0.224034  2.798857  5.320766  4.003479   \n",
       "0      003   t5 -0.015260  0.166744  0.346884  2.209762  3.790535  2.972422   \n",
       "0      005   t3  0.087266  0.028885 -0.076025  3.248507  4.961946  4.170758   \n",
       "0      008   t2  0.503004  0.458838 -0.240136  2.819035  5.837738  3.880701   \n",
       "..     ...  ...       ...       ...       ...       ...       ...       ...   \n",
       "0      005   t5  0.628790  0.897065 -0.344902  2.325528  5.208260  3.783169   \n",
       "0      002   t2  0.326944  0.419992 -0.056302  2.622484  4.641601  3.524013   \n",
       "0      002   t4  0.273119  0.244724 -0.150913  2.567246  4.814949  3.697906   \n",
       "0      002   t6  0.469124  0.572494 -0.068472  2.393223  4.824264  3.476828   \n",
       "0      005   t1  0.103125  0.061807 -0.070540  3.184169  5.127730  4.199516   \n",
       "\n",
       "    e_median     e_std  e_peak_count  e_peak_count_pm     b_min     b_max  \\\n",
       "0   2.888242  0.304057           113        30.678733  1.184541  3.732305   \n",
       "0   3.777844  0.522086           147        25.789474  0.622869  5.320766   \n",
       "0   2.975860  0.299366            93        27.623762 -0.104596  3.790535   \n",
       "0   4.238172  0.352659            79        32.689655  0.952023  4.961946   \n",
       "0   3.895067  0.659222           135        37.327189  1.178414  5.837738   \n",
       "..       ...       ...           ...              ...       ...       ...   \n",
       "0   3.531106  0.641257           159        39.750000  0.965752  5.208260   \n",
       "0   3.450530  0.356463           143        30.211268  1.946925  4.641601   \n",
       "0   3.746131  0.478354           102        35.375723  1.032171  4.814949   \n",
       "0   3.365062  0.404711           126        22.500000  0.214578  4.824264   \n",
       "0   4.293928  0.403393            77        30.394737  1.045372  5.127730   \n",
       "\n",
       "      b_mean  b_median     b_std  b_peak_count  b_peak_count_pm  label  \n",
       "0   3.130351  3.147694  0.215231            71        35.500000      0  \n",
       "0   4.648087  4.682785  0.298052            33        16.363636      0  \n",
       "0   2.957162  3.142604  0.646250            72        35.702479      0  \n",
       "0   4.258024  4.267057  0.276634            67        33.223140      0  \n",
       "0   4.383705  4.353905  0.419087           117        58.016529      1  \n",
       "..       ...       ...       ...           ...              ...    ...  \n",
       "0   4.411959  4.428171  0.296355           110        55.000000      0  \n",
       "0   3.850957  3.870521  0.300161            64        32.000000      1  \n",
       "0   3.971025  3.990854  0.327441            83        41.500000      0  \n",
       "0   3.945952  3.937556  0.336239            58        28.760331      1  \n",
       "0   4.302642  4.355734  0.332853            66        33.000000      0  \n",
       "\n",
       "[64 rows x 20 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>task</th>\n",
       "      <th>d_mean</th>\n",
       "      <th>d_median</th>\n",
       "      <th>d_std</th>\n",
       "      <th>e_min</th>\n",
       "      <th>e_max</th>\n",
       "      <th>e_mean</th>\n",
       "      <th>e_median</th>\n",
       "      <th>e_std</th>\n",
       "      <th>e_peak_count</th>\n",
       "      <th>e_peak_count_pm</th>\n",
       "      <th>b_min</th>\n",
       "      <th>b_max</th>\n",
       "      <th>b_mean</th>\n",
       "      <th>b_median</th>\n",
       "      <th>b_std</th>\n",
       "      <th>b_peak_count</th>\n",
       "      <th>b_peak_count_pm</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003</td>\n",
       "      <td>t2</td>\n",
       "      <td>0.220919</td>\n",
       "      <td>0.259452</td>\n",
       "      <td>-0.088826</td>\n",
       "      <td>2.173757</td>\n",
       "      <td>3.732305</td>\n",
       "      <td>2.909432</td>\n",
       "      <td>2.888242</td>\n",
       "      <td>0.304057</td>\n",
       "      <td>113</td>\n",
       "      <td>30.678733</td>\n",
       "      <td>1.184541</td>\n",
       "      <td>3.732305</td>\n",
       "      <td>3.130351</td>\n",
       "      <td>3.147694</td>\n",
       "      <td>0.215231</td>\n",
       "      <td>71</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>t8</td>\n",
       "      <td>0.644607</td>\n",
       "      <td>0.904941</td>\n",
       "      <td>-0.224034</td>\n",
       "      <td>2.798857</td>\n",
       "      <td>5.320766</td>\n",
       "      <td>4.003479</td>\n",
       "      <td>3.777844</td>\n",
       "      <td>0.522086</td>\n",
       "      <td>147</td>\n",
       "      <td>25.789474</td>\n",
       "      <td>0.622869</td>\n",
       "      <td>5.320766</td>\n",
       "      <td>4.648087</td>\n",
       "      <td>4.682785</td>\n",
       "      <td>0.298052</td>\n",
       "      <td>33</td>\n",
       "      <td>16.363636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003</td>\n",
       "      <td>t5</td>\n",
       "      <td>-0.015260</td>\n",
       "      <td>0.166744</td>\n",
       "      <td>0.346884</td>\n",
       "      <td>2.209762</td>\n",
       "      <td>3.790535</td>\n",
       "      <td>2.972422</td>\n",
       "      <td>2.975860</td>\n",
       "      <td>0.299366</td>\n",
       "      <td>93</td>\n",
       "      <td>27.623762</td>\n",
       "      <td>-0.104596</td>\n",
       "      <td>3.790535</td>\n",
       "      <td>2.957162</td>\n",
       "      <td>3.142604</td>\n",
       "      <td>0.646250</td>\n",
       "      <td>72</td>\n",
       "      <td>35.702479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005</td>\n",
       "      <td>t3</td>\n",
       "      <td>0.087266</td>\n",
       "      <td>0.028885</td>\n",
       "      <td>-0.076025</td>\n",
       "      <td>3.248507</td>\n",
       "      <td>4.961946</td>\n",
       "      <td>4.170758</td>\n",
       "      <td>4.238172</td>\n",
       "      <td>0.352659</td>\n",
       "      <td>79</td>\n",
       "      <td>32.689655</td>\n",
       "      <td>0.952023</td>\n",
       "      <td>4.961946</td>\n",
       "      <td>4.258024</td>\n",
       "      <td>4.267057</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>67</td>\n",
       "      <td>33.223140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>008</td>\n",
       "      <td>t2</td>\n",
       "      <td>0.503004</td>\n",
       "      <td>0.458838</td>\n",
       "      <td>-0.240136</td>\n",
       "      <td>2.819035</td>\n",
       "      <td>5.837738</td>\n",
       "      <td>3.880701</td>\n",
       "      <td>3.895067</td>\n",
       "      <td>0.659222</td>\n",
       "      <td>135</td>\n",
       "      <td>37.327189</td>\n",
       "      <td>1.178414</td>\n",
       "      <td>5.837738</td>\n",
       "      <td>4.383705</td>\n",
       "      <td>4.353905</td>\n",
       "      <td>0.419087</td>\n",
       "      <td>117</td>\n",
       "      <td>58.016529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005</td>\n",
       "      <td>t5</td>\n",
       "      <td>0.628790</td>\n",
       "      <td>0.897065</td>\n",
       "      <td>-0.344902</td>\n",
       "      <td>2.325528</td>\n",
       "      <td>5.208260</td>\n",
       "      <td>3.783169</td>\n",
       "      <td>3.531106</td>\n",
       "      <td>0.641257</td>\n",
       "      <td>159</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>0.965752</td>\n",
       "      <td>5.208260</td>\n",
       "      <td>4.411959</td>\n",
       "      <td>4.428171</td>\n",
       "      <td>0.296355</td>\n",
       "      <td>110</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002</td>\n",
       "      <td>t2</td>\n",
       "      <td>0.326944</td>\n",
       "      <td>0.419992</td>\n",
       "      <td>-0.056302</td>\n",
       "      <td>2.622484</td>\n",
       "      <td>4.641601</td>\n",
       "      <td>3.524013</td>\n",
       "      <td>3.450530</td>\n",
       "      <td>0.356463</td>\n",
       "      <td>143</td>\n",
       "      <td>30.211268</td>\n",
       "      <td>1.946925</td>\n",
       "      <td>4.641601</td>\n",
       "      <td>3.850957</td>\n",
       "      <td>3.870521</td>\n",
       "      <td>0.300161</td>\n",
       "      <td>64</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002</td>\n",
       "      <td>t4</td>\n",
       "      <td>0.273119</td>\n",
       "      <td>0.244724</td>\n",
       "      <td>-0.150913</td>\n",
       "      <td>2.567246</td>\n",
       "      <td>4.814949</td>\n",
       "      <td>3.697906</td>\n",
       "      <td>3.746131</td>\n",
       "      <td>0.478354</td>\n",
       "      <td>102</td>\n",
       "      <td>35.375723</td>\n",
       "      <td>1.032171</td>\n",
       "      <td>4.814949</td>\n",
       "      <td>3.971025</td>\n",
       "      <td>3.990854</td>\n",
       "      <td>0.327441</td>\n",
       "      <td>83</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002</td>\n",
       "      <td>t6</td>\n",
       "      <td>0.469124</td>\n",
       "      <td>0.572494</td>\n",
       "      <td>-0.068472</td>\n",
       "      <td>2.393223</td>\n",
       "      <td>4.824264</td>\n",
       "      <td>3.476828</td>\n",
       "      <td>3.365062</td>\n",
       "      <td>0.404711</td>\n",
       "      <td>126</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>0.214578</td>\n",
       "      <td>4.824264</td>\n",
       "      <td>3.945952</td>\n",
       "      <td>3.937556</td>\n",
       "      <td>0.336239</td>\n",
       "      <td>58</td>\n",
       "      <td>28.760331</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005</td>\n",
       "      <td>t1</td>\n",
       "      <td>0.103125</td>\n",
       "      <td>0.061807</td>\n",
       "      <td>-0.070540</td>\n",
       "      <td>3.184169</td>\n",
       "      <td>5.127730</td>\n",
       "      <td>4.199516</td>\n",
       "      <td>4.293928</td>\n",
       "      <td>0.403393</td>\n",
       "      <td>77</td>\n",
       "      <td>30.394737</td>\n",
       "      <td>1.045372</td>\n",
       "      <td>5.127730</td>\n",
       "      <td>4.302642</td>\n",
       "      <td>4.355734</td>\n",
       "      <td>0.332853</td>\n",
       "      <td>66</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 20 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "TRAIN_FILTER = (~(dataset['task'] == 't7') & ~(dataset['subject'] == '007'))\n",
    "\n",
    "data_train = dataset[~TRAIN_FILTER].drop(['subject', 'task', 'label'], axis=1).to_numpy()\n",
    "data_test = dataset[TRAIN_FILTER].drop(['subject', 'task', 'label'], axis=1).to_numpy()\n",
    "\n",
    "targets_train = dataset[~TRAIN_FILTER]['label'].to_numpy()\n",
    "targets_test = dataset[TRAIN_FILTER]['label'].to_numpy()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# test shapes\n",
    "print(data_train.shape)\n",
    "print(targets_train.shape)\n",
    "print(data_test.shape)\n",
    "print(targets_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15, 17)\n",
      "(15,)\n",
      "(49, 17)\n",
      "(49,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "## wrap data in dataset\n",
    "# cast to tensor\n",
    "data_train = torch.tensor(data_train).float()\n",
    "targets_train = torch.tensor(targets_train).long()#.unsqueeze(dim=-1)\n",
    "data_test = torch.tensor(data_test).float()\n",
    "targets_test = torch.tensor(targets_test).long()#.unsqueeze(dim=-1)\n",
    "# wrap in dataset class\n",
    "dataset_train = torch.utils.data.TensorDataset(data_train, targets_train)\n",
    "dataset_test = torch.utils.data.TensorDataset(data_test, targets_test)\n",
    "# create dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(dataset_train, batch_size=10,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset_test, batch_size=10,shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## create model\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, input_dimension,number_of_target_classes):\n",
    "        super().__init__()\n",
    "        # stack layers\n",
    "        self.module_list = torch.nn.ModuleList()\n",
    "        ## stacks contain of linear + nonlinear layers\n",
    "        self.module_list.append(torch.nn.Linear(input_dimension,20))\n",
    "        self.module_list.append(torch.nn.Tanh())\n",
    "        #\n",
    "        self.module_list.append(torch.nn.Linear(20,30))\n",
    "        self.module_list.append(torch.nn.Tanh())\n",
    "        #\n",
    "        self.module_list.append(torch.nn.Linear(30,10))\n",
    "        self.module_list.append(torch.nn.Tanh())\n",
    "        # map to output layer\n",
    "        self.module_list.append(torch.nn.Linear(10,number_of_target_classes))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#instanciate model\n",
    "model = NN(17,2)\n",
    "#model.to(device)\n",
    "# test forward pass\n",
    "model(data_train[:10])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.1967,  0.2130],\n",
       "        [-0.1974,  0.2055],\n",
       "        [-0.1691,  0.4124],\n",
       "        [-0.0540,  0.2461],\n",
       "        [-0.2016,  0.1782],\n",
       "        [-0.1036,  0.3670],\n",
       "        [-0.2010,  0.1813],\n",
       "        [-0.2050,  0.1615],\n",
       "        [-0.1847,  0.2038],\n",
       "        [-0.2056,  0.1477]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# def criterion and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# train\n",
    "for epoch in range(2000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, targets = data\n",
    "        inputs.to(device)\n",
    "        targets.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        # \n",
    "        targets = targets.squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 0:    # print every 2000 mini-batches\n",
    "            #print(\"Training loss {} Steps: {}\".format(running_loss / 1000, epoch * len(trainloader) + i))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "new task\n",
    "new participant\n",
    "new task new participant"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in testloader:\n",
    "        output = model(inputs) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = ('LOW', 'HIGH')\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix), index = [i for i in classes], columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('without-task-7-subject-7.png')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "torch.save(model.state_dict(), './export/torch')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   subject task    d_mean  d_median     d_std     e_min     e_max    e_mean  \\\n",
       "0      003   t2  0.220919  0.259452 -0.088826  2.173757  3.732305  2.909432   \n",
       "0      001   t8  0.644607  0.904941 -0.224034  2.798857  5.320766  4.003479   \n",
       "0      003   t5 -0.015260  0.166744  0.346884  2.209762  3.790535  2.972422   \n",
       "0      005   t3  0.087266  0.028885 -0.076025  3.248507  4.961946  4.170758   \n",
       "0      008   t2  0.503004  0.458838 -0.240136  2.819035  5.837738  3.880701   \n",
       "..     ...  ...       ...       ...       ...       ...       ...       ...   \n",
       "0      005   t5  0.628790  0.897065 -0.344902  2.325528  5.208260  3.783169   \n",
       "0      002   t2  0.326944  0.419992 -0.056302  2.622484  4.641601  3.524013   \n",
       "0      002   t4  0.273119  0.244724 -0.150913  2.567246  4.814949  3.697906   \n",
       "0      002   t6  0.469124  0.572494 -0.068472  2.393223  4.824264  3.476828   \n",
       "0      005   t1  0.103125  0.061807 -0.070540  3.184169  5.127730  4.199516   \n",
       "\n",
       "    e_median     e_std  e_peak_count  e_peak_count_pm     b_min     b_max  \\\n",
       "0   2.888242  0.304057           113        30.678733  1.184541  3.732305   \n",
       "0   3.777844  0.522086           147        25.789474  0.622869  5.320766   \n",
       "0   2.975860  0.299366            93        27.623762 -0.104596  3.790535   \n",
       "0   4.238172  0.352659            79        32.689655  0.952023  4.961946   \n",
       "0   3.895067  0.659222           135        37.327189  1.178414  5.837738   \n",
       "..       ...       ...           ...              ...       ...       ...   \n",
       "0   3.531106  0.641257           159        39.750000  0.965752  5.208260   \n",
       "0   3.450530  0.356463           143        30.211268  1.946925  4.641601   \n",
       "0   3.746131  0.478354           102        35.375723  1.032171  4.814949   \n",
       "0   3.365062  0.404711           126        22.500000  0.214578  4.824264   \n",
       "0   4.293928  0.403393            77        30.394737  1.045372  5.127730   \n",
       "\n",
       "      b_mean  b_median     b_std  b_peak_count  b_peak_count_pm  label  \n",
       "0   3.130351  3.147694  0.215231            71        35.500000      0  \n",
       "0   4.648087  4.682785  0.298052            33        16.363636      0  \n",
       "0   2.957162  3.142604  0.646250            72        35.702479      0  \n",
       "0   4.258024  4.267057  0.276634            67        33.223140      0  \n",
       "0   4.383705  4.353905  0.419087           117        58.016529      1  \n",
       "..       ...       ...       ...           ...              ...    ...  \n",
       "0   4.411959  4.428171  0.296355           110        55.000000      0  \n",
       "0   3.850957  3.870521  0.300161            64        32.000000      1  \n",
       "0   3.971025  3.990854  0.327441            83        41.500000      0  \n",
       "0   3.945952  3.937556  0.336239            58        28.760331      1  \n",
       "0   4.302642  4.355734  0.332853            66        33.000000      0  \n",
       "\n",
       "[64 rows x 20 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>task</th>\n",
       "      <th>d_mean</th>\n",
       "      <th>d_median</th>\n",
       "      <th>d_std</th>\n",
       "      <th>e_min</th>\n",
       "      <th>e_max</th>\n",
       "      <th>e_mean</th>\n",
       "      <th>e_median</th>\n",
       "      <th>e_std</th>\n",
       "      <th>e_peak_count</th>\n",
       "      <th>e_peak_count_pm</th>\n",
       "      <th>b_min</th>\n",
       "      <th>b_max</th>\n",
       "      <th>b_mean</th>\n",
       "      <th>b_median</th>\n",
       "      <th>b_std</th>\n",
       "      <th>b_peak_count</th>\n",
       "      <th>b_peak_count_pm</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003</td>\n",
       "      <td>t2</td>\n",
       "      <td>0.220919</td>\n",
       "      <td>0.259452</td>\n",
       "      <td>-0.088826</td>\n",
       "      <td>2.173757</td>\n",
       "      <td>3.732305</td>\n",
       "      <td>2.909432</td>\n",
       "      <td>2.888242</td>\n",
       "      <td>0.304057</td>\n",
       "      <td>113</td>\n",
       "      <td>30.678733</td>\n",
       "      <td>1.184541</td>\n",
       "      <td>3.732305</td>\n",
       "      <td>3.130351</td>\n",
       "      <td>3.147694</td>\n",
       "      <td>0.215231</td>\n",
       "      <td>71</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>t8</td>\n",
       "      <td>0.644607</td>\n",
       "      <td>0.904941</td>\n",
       "      <td>-0.224034</td>\n",
       "      <td>2.798857</td>\n",
       "      <td>5.320766</td>\n",
       "      <td>4.003479</td>\n",
       "      <td>3.777844</td>\n",
       "      <td>0.522086</td>\n",
       "      <td>147</td>\n",
       "      <td>25.789474</td>\n",
       "      <td>0.622869</td>\n",
       "      <td>5.320766</td>\n",
       "      <td>4.648087</td>\n",
       "      <td>4.682785</td>\n",
       "      <td>0.298052</td>\n",
       "      <td>33</td>\n",
       "      <td>16.363636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003</td>\n",
       "      <td>t5</td>\n",
       "      <td>-0.015260</td>\n",
       "      <td>0.166744</td>\n",
       "      <td>0.346884</td>\n",
       "      <td>2.209762</td>\n",
       "      <td>3.790535</td>\n",
       "      <td>2.972422</td>\n",
       "      <td>2.975860</td>\n",
       "      <td>0.299366</td>\n",
       "      <td>93</td>\n",
       "      <td>27.623762</td>\n",
       "      <td>-0.104596</td>\n",
       "      <td>3.790535</td>\n",
       "      <td>2.957162</td>\n",
       "      <td>3.142604</td>\n",
       "      <td>0.646250</td>\n",
       "      <td>72</td>\n",
       "      <td>35.702479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005</td>\n",
       "      <td>t3</td>\n",
       "      <td>0.087266</td>\n",
       "      <td>0.028885</td>\n",
       "      <td>-0.076025</td>\n",
       "      <td>3.248507</td>\n",
       "      <td>4.961946</td>\n",
       "      <td>4.170758</td>\n",
       "      <td>4.238172</td>\n",
       "      <td>0.352659</td>\n",
       "      <td>79</td>\n",
       "      <td>32.689655</td>\n",
       "      <td>0.952023</td>\n",
       "      <td>4.961946</td>\n",
       "      <td>4.258024</td>\n",
       "      <td>4.267057</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>67</td>\n",
       "      <td>33.223140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>008</td>\n",
       "      <td>t2</td>\n",
       "      <td>0.503004</td>\n",
       "      <td>0.458838</td>\n",
       "      <td>-0.240136</td>\n",
       "      <td>2.819035</td>\n",
       "      <td>5.837738</td>\n",
       "      <td>3.880701</td>\n",
       "      <td>3.895067</td>\n",
       "      <td>0.659222</td>\n",
       "      <td>135</td>\n",
       "      <td>37.327189</td>\n",
       "      <td>1.178414</td>\n",
       "      <td>5.837738</td>\n",
       "      <td>4.383705</td>\n",
       "      <td>4.353905</td>\n",
       "      <td>0.419087</td>\n",
       "      <td>117</td>\n",
       "      <td>58.016529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005</td>\n",
       "      <td>t5</td>\n",
       "      <td>0.628790</td>\n",
       "      <td>0.897065</td>\n",
       "      <td>-0.344902</td>\n",
       "      <td>2.325528</td>\n",
       "      <td>5.208260</td>\n",
       "      <td>3.783169</td>\n",
       "      <td>3.531106</td>\n",
       "      <td>0.641257</td>\n",
       "      <td>159</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>0.965752</td>\n",
       "      <td>5.208260</td>\n",
       "      <td>4.411959</td>\n",
       "      <td>4.428171</td>\n",
       "      <td>0.296355</td>\n",
       "      <td>110</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002</td>\n",
       "      <td>t2</td>\n",
       "      <td>0.326944</td>\n",
       "      <td>0.419992</td>\n",
       "      <td>-0.056302</td>\n",
       "      <td>2.622484</td>\n",
       "      <td>4.641601</td>\n",
       "      <td>3.524013</td>\n",
       "      <td>3.450530</td>\n",
       "      <td>0.356463</td>\n",
       "      <td>143</td>\n",
       "      <td>30.211268</td>\n",
       "      <td>1.946925</td>\n",
       "      <td>4.641601</td>\n",
       "      <td>3.850957</td>\n",
       "      <td>3.870521</td>\n",
       "      <td>0.300161</td>\n",
       "      <td>64</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002</td>\n",
       "      <td>t4</td>\n",
       "      <td>0.273119</td>\n",
       "      <td>0.244724</td>\n",
       "      <td>-0.150913</td>\n",
       "      <td>2.567246</td>\n",
       "      <td>4.814949</td>\n",
       "      <td>3.697906</td>\n",
       "      <td>3.746131</td>\n",
       "      <td>0.478354</td>\n",
       "      <td>102</td>\n",
       "      <td>35.375723</td>\n",
       "      <td>1.032171</td>\n",
       "      <td>4.814949</td>\n",
       "      <td>3.971025</td>\n",
       "      <td>3.990854</td>\n",
       "      <td>0.327441</td>\n",
       "      <td>83</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002</td>\n",
       "      <td>t6</td>\n",
       "      <td>0.469124</td>\n",
       "      <td>0.572494</td>\n",
       "      <td>-0.068472</td>\n",
       "      <td>2.393223</td>\n",
       "      <td>4.824264</td>\n",
       "      <td>3.476828</td>\n",
       "      <td>3.365062</td>\n",
       "      <td>0.404711</td>\n",
       "      <td>126</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>0.214578</td>\n",
       "      <td>4.824264</td>\n",
       "      <td>3.945952</td>\n",
       "      <td>3.937556</td>\n",
       "      <td>0.336239</td>\n",
       "      <td>58</td>\n",
       "      <td>28.760331</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005</td>\n",
       "      <td>t1</td>\n",
       "      <td>0.103125</td>\n",
       "      <td>0.061807</td>\n",
       "      <td>-0.070540</td>\n",
       "      <td>3.184169</td>\n",
       "      <td>5.127730</td>\n",
       "      <td>4.199516</td>\n",
       "      <td>4.293928</td>\n",
       "      <td>0.403393</td>\n",
       "      <td>77</td>\n",
       "      <td>30.394737</td>\n",
       "      <td>1.045372</td>\n",
       "      <td>5.127730</td>\n",
       "      <td>4.302642</td>\n",
       "      <td>4.355734</td>\n",
       "      <td>0.332853</td>\n",
       "      <td>66</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 20 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "right = 0\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    device = torch.device('cuda')\n",
    "    model2 = NN(17,2)\n",
    "    model2.load_state_dict(torch.load('./export/torch', map_location=device))\n",
    "    model2.eval()\n",
    "\n",
    "    for index in list(range(1, 64)):\n",
    "        data_to_predict = dataset.drop(['subject', 'task', 'label'], axis=1).to_numpy()[index]\n",
    "\n",
    "        prediction = model2(torch.from_numpy(data_to_predict).float())\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        print(prediction, predicted_class, dataset.iloc[index]['label'], dataset.iloc[index]['subject'], dataset.iloc[index]['task'])\n",
    "        right += 1 if predicted_class == dataset.iloc[index]['label'] else 0\n",
    "        count += 1\n",
    "\n",
    "print(right, count, right/count)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-1.9145,  2.4478]) tensor(1) 0 001 t8\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 003 t5\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 005 t3\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 008 t2\n",
      "tensor([-1.9145,  2.4479]) tensor(1) 0 001 t5\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 008 t7\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 002 t5\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 005 t7\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 003 t3\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 002 t8\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 008 t1\n",
      "tensor([ 0.6646, -0.6851]) tensor(0) 0 003 t6\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 004 t8\n",
      "tensor([ 0.6645, -0.6851]) tensor(0) 0 006 t7\n",
      "tensor([-1.9145,  2.4481]) tensor(1) 1 001 t7\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 007 t1\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 005 t4\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 006 t3\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 004 t3\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 008 t4\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 006 t8\n",
      "tensor([ 0.6458, -0.6604]) tensor(0) 0 001 t1\n",
      "tensor([-1.9145,  2.4481]) tensor(1) 1 007 t6\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 001 t3\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 006 t2\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 006 t5\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 007 t4\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 002 t3\n",
      "tensor([ 0.6647, -0.6853]) tensor(0) 0 007 t3\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 008 t6\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 004 t1\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 004 t4\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 004 t7\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 005 t8\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 008 t3\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 003 t7\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 004 t5\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 003 t4\n",
      "tensor([ 0.6645, -0.6850]) tensor(0) 0 006 t6\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 002 t7\n",
      "tensor([-1.9145,  2.4481]) tensor(1) 0 004 t2\n",
      "tensor([-1.8779,  2.4024]) tensor(1) 1 007 t8\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 005 t2\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 008 t5\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 003 t1\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 008 t8\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 007 t5\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 005 t6\n",
      "tensor([ 0.6647, -0.6853]) tensor(0) 0 007 t7\n",
      "tensor([-1.9145,  2.4481]) tensor(1) 1 001 t6\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 002 t1\n",
      "tensor([-1.9145,  2.4481]) tensor(1) 0 003 t8\n",
      "tensor([-1.8804,  2.3378]) tensor(1) 0 001 t2\n",
      "tensor([ 0.6647, -0.6852]) tensor(0) 0 006 t4\n",
      "tensor([-1.9145,  2.4481]) tensor(1) 0 004 t6\n",
      "tensor([ 0.6631, -0.6832]) tensor(0) 0 007 t2\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 001 t4\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 006 t1\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 005 t5\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 002 t2\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 002 t4\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 1 002 t6\n",
      "tensor([ 0.6648, -0.6853]) tensor(0) 0 005 t1\n",
      "45 63 0.7142857142857143\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model's state_dict:\n",
      "module_list.0.weight \t torch.Size([20, 17])\n",
      "module_list.0.bias \t torch.Size([20])\n",
      "module_list.2.weight \t torch.Size([30, 20])\n",
      "module_list.2.bias \t torch.Size([30])\n",
      "module_list.4.weight \t torch.Size([10, 30])\n",
      "module_list.4.bias \t torch.Size([10])\n",
      "module_list.6.weight \t torch.Size([2, 10])\n",
      "module_list.6.bias \t torch.Size([2])\n",
      "Optimizer's state_dict:\n",
      "state \t {0: {'momentum_buffer': tensor([[ 3.3472e-10,  4.1225e-10, -1.4525e-10,  3.2007e-09,  5.3844e-09,\n",
      "          4.1221e-09,  4.0503e-09,  3.9661e-10,  5.0631e-08,  1.2450e-08,\n",
      "          1.2871e-09,  5.3844e-09,  4.4568e-09,  4.4625e-09,  2.5136e-10,\n",
      "          2.5965e-08,  1.2982e-08],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-3.0062e-08, -1.4696e-08,  1.8600e-08, -4.8114e-07, -8.3098e-07,\n",
      "         -6.3562e-07, -6.5231e-07, -6.6066e-08, -8.9893e-06, -3.1953e-06,\n",
      "         -2.3230e-07, -8.3098e-07, -6.6568e-07, -6.6700e-07, -4.7465e-08,\n",
      "         -7.1476e-06, -3.5729e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.2054e-07,  1.4813e-07, -5.2386e-08,  1.1534e-06,  1.9411e-06,\n",
      "          1.4858e-06,  1.4603e-06,  1.4313e-07,  1.8276e-05,  4.5049e-06,\n",
      "          4.6429e-07,  1.9411e-06,  1.6064e-06,  1.6084e-06,  9.0747e-08,\n",
      "          9.4065e-06,  4.7030e-06],\n",
      "        [ 3.7689e-08,  4.6366e-08, -1.6366e-08,  3.6043e-07,  6.0648e-07,\n",
      "          4.6426e-07,  4.5623e-07,  4.4701e-08,  5.7081e-06,  1.4052e-06,\n",
      "          1.4501e-07,  6.0648e-07,  5.0195e-07,  5.0260e-07,  2.8335e-08,\n",
      "          2.9325e-06,  1.4662e-06],\n",
      "        [-8.4741e-08, -1.0277e-07,  3.7152e-08, -8.2134e-07, -1.3841e-06,\n",
      "         -1.0592e-06, -1.0427e-06, -1.0244e-07, -1.3115e-05, -3.2904e-06,\n",
      "         -3.3298e-07, -1.3841e-06, -1.1440e-06, -1.1455e-06, -6.5287e-08,\n",
      "         -6.9070e-06, -3.4532e-06],\n",
      "        [-8.1821e-06, -1.0086e-05,  8.3955e-07, -1.8969e-05, -5.6273e-05,\n",
      "         -3.7065e-05, -3.4909e-05, -5.4794e-06, -1.8196e-03, -2.0658e-04,\n",
      "         -3.9813e-05, -5.6273e-05, -4.5247e-05, -4.4995e-05, -4.6398e-06,\n",
      "         -5.0543e-04, -2.5534e-04],\n",
      "        [ 1.9216e-10,  2.3667e-10, -8.3386e-11,  1.8375e-09,  3.0911e-09,\n",
      "          2.3664e-09,  2.3252e-09,  2.2769e-10,  2.9067e-08,  7.1476e-09,\n",
      "          7.3889e-10,  3.0911e-09,  2.5586e-09,  2.5619e-09,  1.4430e-10,\n",
      "          1.4906e-08,  7.4530e-09],\n",
      "        [-1.0749e-07, -1.2278e-07,  4.8991e-08, -1.0918e-06, -1.8499e-06,\n",
      "         -1.4143e-06, -1.4012e-06, -1.3916e-07, -1.7927e-05, -4.7974e-06,\n",
      "         -4.5509e-07, -1.8499e-06, -1.5218e-06, -1.5239e-06, -9.0172e-08,\n",
      "         -1.0264e-05, -5.1300e-06],\n",
      "        [-2.8985e-08, -3.5058e-08,  1.2736e-08, -2.8221e-07, -4.7548e-07,\n",
      "         -3.6395e-07, -3.5839e-07, -3.5185e-08, -4.5054e-06, -1.1352e-06,\n",
      "         -1.1456e-07, -4.7548e-07, -3.9293e-07, -3.9345e-07, -2.2449e-08,\n",
      "         -2.3830e-06, -1.1914e-06],\n",
      "        [ 7.3761e-07,  4.3855e-07, -4.3593e-07,  1.0512e-05,  1.8257e-05,\n",
      "          1.3913e-05,  1.4241e-05,  1.4719e-06,  1.9750e-04,  6.8193e-05,\n",
      "          5.0178e-06,  1.8257e-05,  1.4651e-05,  1.4680e-05,  1.0360e-06,\n",
      "          1.5380e-04,  7.6825e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 3.2039e-09,  3.9460e-09, -1.3903e-09,  3.0636e-08,  5.1538e-08,\n",
      "          3.9456e-08,  3.8768e-08,  3.7962e-09,  4.8463e-07,  1.1917e-07,\n",
      "          1.2319e-08,  5.1538e-08,  4.2660e-08,  4.2714e-08,  2.4059e-09,\n",
      "          2.4853e-07,  1.2426e-07],\n",
      "        [-3.1067e-08, -3.8322e-08,  1.3418e-08, -2.9686e-07, -5.0000e-07,\n",
      "         -3.8266e-07, -3.7592e-07, -3.6819e-08, -4.7746e-06, -1.1715e-06,\n",
      "         -1.1901e-07, -5.0000e-07, -4.1372e-07, -4.1424e-07, -2.3402e-08,\n",
      "         -2.4359e-06, -1.2176e-06],\n",
      "        [-4.1406e-08, -2.5593e-08,  2.3960e-08, -5.3602e-07, -9.4304e-07,\n",
      "         -7.1440e-07, -7.3179e-07, -7.8438e-08, -1.0640e-05, -3.6085e-06,\n",
      "         -2.5328e-07, -9.4304e-07, -7.5580e-07, -7.5739e-07, -5.4478e-08,\n",
      "         -8.3829e-06, -4.1817e-06],\n",
      "        [ 4.9548e-09,  2.8777e-09, -2.9476e-09,  7.1618e-08,  1.2429e-07,\n",
      "          9.4772e-08,  9.7048e-08,  1.0007e-08,  1.3461e-06,  4.6665e-07,\n",
      "          3.4263e-08,  1.2429e-07,  9.9727e-08,  9.9926e-08,  7.0595e-09,\n",
      "          1.0509e-06,  5.2497e-07],\n",
      "        [ 2.0455e-09,  1.1999e-09, -1.2108e-09,  2.7846e-08,  4.8719e-08,\n",
      "          3.7000e-08,  3.7924e-08,  4.0011e-09,  5.3342e-07,  1.8373e-07,\n",
      "          1.3361e-08,  4.8719e-08,  3.9045e-08,  3.9124e-08,  2.7903e-09,\n",
      "          4.1855e-07,  2.0893e-07],\n",
      "        [-3.5335e-07, -2.4073e-07,  2.0006e-07, -4.1728e-06, -7.3795e-06,\n",
      "         -5.5693e-06, -5.6940e-06, -6.2172e-07, -8.1566e-05, -2.7058e-05,\n",
      "         -1.9762e-06, -7.3795e-06, -5.9227e-06, -5.9347e-06, -4.2167e-07,\n",
      "         -6.2725e-05, -3.1270e-05],\n",
      "        [-5.8151e-07, -3.7048e-07,  3.3667e-07, -7.5110e-06, -1.3169e-05,\n",
      "         -9.9851e-06, -1.0217e-05, -1.0869e-06, -1.4397e-04, -4.8782e-05,\n",
      "         -3.5782e-06, -1.3169e-05, -1.0567e-05, -1.0588e-05, -7.5026e-07,\n",
      "         -1.1149e-04, -5.5632e-05]])}, 1: {'momentum_buffer': tensor([ 1.2982e-09,  0.0000e+00, -1.6611e-07,  0.0000e+00,  4.6754e-07,\n",
      "         1.4615e-07, -3.3115e-07, -1.0481e-05,  7.4530e-10, -4.3158e-07,\n",
      "        -1.1367e-07,  3.6932e-06,  0.0000e+00,  1.2426e-08, -1.2016e-07,\n",
      "        -1.9007e-07,  2.5101e-08,  9.7886e-09, -1.4993e-06, -2.6666e-06])}, 2: {'momentum_buffer': tensor([[-0.0172, -0.0172, -0.0172,  0.0172, -0.0172,  0.0172, -0.0172, -0.0160,\n",
      "          0.0172,  0.0172,  0.0172, -0.0172,  0.0172,  0.0172, -0.0172, -0.0172,\n",
      "          0.0172,  0.0172,  0.0172,  0.0172],\n",
      "        [ 0.0031,  0.0031,  0.0031, -0.0031,  0.0031, -0.0031,  0.0031,  0.0030,\n",
      "         -0.0031, -0.0031, -0.0031,  0.0031, -0.0031, -0.0031,  0.0031,  0.0031,\n",
      "         -0.0031, -0.0031, -0.0031, -0.0031],\n",
      "        [ 0.0091,  0.0091,  0.0091, -0.0091,  0.0091, -0.0091,  0.0091,  0.0087,\n",
      "         -0.0091, -0.0091, -0.0091,  0.0091, -0.0091, -0.0091,  0.0091,  0.0091,\n",
      "         -0.0091, -0.0091, -0.0091, -0.0091],\n",
      "        [-0.0216, -0.0216, -0.0216,  0.0216, -0.0217,  0.0217, -0.0216, -0.0200,\n",
      "          0.0216,  0.0217,  0.0216, -0.0217,  0.0216,  0.0216, -0.0216, -0.0216,\n",
      "          0.0216,  0.0216,  0.0217,  0.0217],\n",
      "        [ 0.0276,  0.0276,  0.0276, -0.0276,  0.0276, -0.0276,  0.0276,  0.0261,\n",
      "         -0.0276, -0.0276, -0.0276,  0.0276, -0.0276, -0.0276,  0.0276,  0.0276,\n",
      "         -0.0276, -0.0276, -0.0276, -0.0276],\n",
      "        [ 0.0228,  0.0228,  0.0228, -0.0228,  0.0228, -0.0228,  0.0228,  0.0215,\n",
      "         -0.0228, -0.0228, -0.0228,  0.0228, -0.0228, -0.0228,  0.0228,  0.0228,\n",
      "         -0.0228, -0.0228, -0.0228, -0.0228],\n",
      "        [-0.0135, -0.0135, -0.0135,  0.0135, -0.0135,  0.0135, -0.0135, -0.0126,\n",
      "          0.0135,  0.0135,  0.0135, -0.0135,  0.0135,  0.0135, -0.0135, -0.0135,\n",
      "          0.0135,  0.0135,  0.0135,  0.0135],\n",
      "        [ 0.0076,  0.0076,  0.0076, -0.0076,  0.0076, -0.0076,  0.0076,  0.0073,\n",
      "         -0.0076, -0.0076, -0.0076,  0.0076, -0.0076, -0.0076,  0.0076,  0.0076,\n",
      "         -0.0076, -0.0076, -0.0076, -0.0076],\n",
      "        [-0.0247, -0.0247, -0.0247,  0.0247, -0.0247,  0.0247, -0.0247, -0.0229,\n",
      "          0.0247,  0.0247,  0.0247, -0.0247,  0.0247,  0.0247, -0.0247, -0.0247,\n",
      "          0.0247,  0.0247,  0.0247,  0.0247],\n",
      "        [-0.0263, -0.0263, -0.0263,  0.0263, -0.0263,  0.0263, -0.0263, -0.0235,\n",
      "          0.0263,  0.0263,  0.0263, -0.0263,  0.0263,  0.0263, -0.0263, -0.0263,\n",
      "          0.0263,  0.0263,  0.0263,  0.0263],\n",
      "        [ 0.0181,  0.0181,  0.0181, -0.0181,  0.0181, -0.0181,  0.0181,  0.0165,\n",
      "         -0.0181, -0.0181, -0.0181,  0.0181, -0.0181, -0.0181,  0.0181,  0.0181,\n",
      "         -0.0181, -0.0181, -0.0181, -0.0181],\n",
      "        [-0.0198, -0.0198, -0.0198,  0.0198, -0.0198,  0.0198, -0.0198, -0.0185,\n",
      "          0.0198,  0.0198,  0.0198, -0.0198,  0.0198,  0.0198, -0.0198, -0.0198,\n",
      "          0.0198,  0.0198,  0.0198,  0.0198],\n",
      "        [-0.0062, -0.0062, -0.0062,  0.0062, -0.0062,  0.0062, -0.0062, -0.0058,\n",
      "          0.0062,  0.0062,  0.0062, -0.0062,  0.0062,  0.0062, -0.0062, -0.0062,\n",
      "          0.0062,  0.0062,  0.0062,  0.0062],\n",
      "        [ 0.0005,  0.0005,  0.0005, -0.0005,  0.0005, -0.0005,  0.0005,  0.0005,\n",
      "         -0.0005, -0.0005, -0.0005,  0.0005, -0.0005, -0.0005,  0.0005,  0.0005,\n",
      "         -0.0005, -0.0005, -0.0005, -0.0005],\n",
      "        [-0.0270, -0.0270, -0.0270,  0.0270, -0.0270,  0.0270, -0.0270, -0.0252,\n",
      "          0.0270,  0.0270,  0.0270, -0.0270,  0.0270,  0.0270, -0.0270, -0.0270,\n",
      "          0.0270,  0.0270,  0.0270,  0.0270],\n",
      "        [-0.0062, -0.0062, -0.0062,  0.0062, -0.0062,  0.0062, -0.0062, -0.0060,\n",
      "          0.0062,  0.0062,  0.0062, -0.0062,  0.0062,  0.0062, -0.0062, -0.0062,\n",
      "          0.0062,  0.0062,  0.0062,  0.0062],\n",
      "        [-0.0259, -0.0259, -0.0259,  0.0259, -0.0259,  0.0259, -0.0259, -0.0245,\n",
      "          0.0259,  0.0259,  0.0259, -0.0259,  0.0259,  0.0259, -0.0259, -0.0259,\n",
      "          0.0259,  0.0259,  0.0259,  0.0259],\n",
      "        [ 0.0172,  0.0172,  0.0172, -0.0172,  0.0172, -0.0172,  0.0172,  0.0157,\n",
      "         -0.0172, -0.0172, -0.0172,  0.0172, -0.0172, -0.0172,  0.0172,  0.0172,\n",
      "         -0.0172, -0.0172, -0.0172, -0.0172],\n",
      "        [ 0.0275,  0.0275,  0.0275, -0.0275,  0.0275, -0.0275,  0.0275,  0.0259,\n",
      "         -0.0275, -0.0275, -0.0275,  0.0275, -0.0275, -0.0275,  0.0275,  0.0275,\n",
      "         -0.0275, -0.0275, -0.0275, -0.0275],\n",
      "        [-0.0042, -0.0042, -0.0042,  0.0042, -0.0042,  0.0042, -0.0042, -0.0041,\n",
      "          0.0042,  0.0042,  0.0042, -0.0042,  0.0042,  0.0042, -0.0042, -0.0042,\n",
      "          0.0042,  0.0042,  0.0042,  0.0042],\n",
      "        [-0.0098, -0.0098, -0.0098,  0.0098, -0.0098,  0.0098, -0.0098, -0.0090,\n",
      "          0.0098,  0.0098,  0.0098, -0.0098,  0.0098,  0.0098, -0.0098, -0.0098,\n",
      "          0.0098,  0.0098,  0.0098,  0.0098],\n",
      "        [ 0.0012,  0.0012,  0.0012, -0.0012,  0.0012, -0.0012,  0.0012,  0.0010,\n",
      "         -0.0012, -0.0012, -0.0012,  0.0012, -0.0012, -0.0012,  0.0012,  0.0012,\n",
      "         -0.0012, -0.0012, -0.0012, -0.0012],\n",
      "        [-0.0238, -0.0238, -0.0238,  0.0238, -0.0238,  0.0238, -0.0238, -0.0223,\n",
      "          0.0238,  0.0238,  0.0238, -0.0238,  0.0238,  0.0238, -0.0238, -0.0238,\n",
      "          0.0238,  0.0238,  0.0238,  0.0238],\n",
      "        [-0.0048, -0.0048, -0.0048,  0.0048, -0.0048,  0.0048, -0.0048, -0.0045,\n",
      "          0.0048,  0.0048,  0.0048, -0.0048,  0.0048,  0.0048, -0.0048, -0.0048,\n",
      "          0.0048,  0.0048,  0.0048,  0.0048],\n",
      "        [ 0.0062,  0.0062,  0.0062, -0.0062,  0.0062, -0.0062,  0.0062,  0.0056,\n",
      "         -0.0062, -0.0062, -0.0062,  0.0062, -0.0062, -0.0062,  0.0062,  0.0062,\n",
      "         -0.0062, -0.0062, -0.0062, -0.0062],\n",
      "        [-0.0075, -0.0075, -0.0075,  0.0075, -0.0075,  0.0075, -0.0075, -0.0068,\n",
      "          0.0075,  0.0075,  0.0075, -0.0075,  0.0075,  0.0075, -0.0075, -0.0075,\n",
      "          0.0075,  0.0075,  0.0075,  0.0075],\n",
      "        [-0.0014, -0.0014, -0.0014,  0.0014, -0.0014,  0.0014, -0.0014, -0.0012,\n",
      "          0.0014,  0.0014,  0.0014, -0.0014,  0.0014,  0.0014, -0.0014, -0.0014,\n",
      "          0.0014,  0.0014,  0.0014,  0.0014],\n",
      "        [-0.0162, -0.0162, -0.0162,  0.0162, -0.0162,  0.0162, -0.0162, -0.0151,\n",
      "          0.0162,  0.0162,  0.0162, -0.0162,  0.0162,  0.0162, -0.0162, -0.0162,\n",
      "          0.0162,  0.0162,  0.0162,  0.0162],\n",
      "        [ 0.0064,  0.0064,  0.0064, -0.0064,  0.0064, -0.0064,  0.0064,  0.0059,\n",
      "         -0.0064, -0.0064, -0.0064,  0.0064, -0.0064, -0.0064,  0.0064,  0.0064,\n",
      "         -0.0064, -0.0064, -0.0064, -0.0064],\n",
      "        [ 0.0112,  0.0112,  0.0112, -0.0112,  0.0112, -0.0112,  0.0112,  0.0105,\n",
      "         -0.0112, -0.0112, -0.0112,  0.0112, -0.0112, -0.0112,  0.0112,  0.0112,\n",
      "         -0.0112, -0.0112, -0.0112, -0.0112]])}, 3: {'momentum_buffer': tensor([ 0.0172, -0.0031, -0.0091,  0.0216, -0.0276, -0.0228,  0.0135, -0.0076,\n",
      "         0.0247,  0.0263, -0.0181,  0.0198,  0.0062, -0.0005,  0.0270,  0.0062,\n",
      "         0.0259, -0.0172, -0.0275,  0.0042,  0.0098, -0.0012,  0.0238,  0.0048,\n",
      "        -0.0062,  0.0075,  0.0014,  0.0162, -0.0064, -0.0112])}, 4: {'momentum_buffer': tensor([[-1.1032e-03,  3.8431e-03, -5.9390e-03, -4.8128e-03,  1.1610e-03,\n",
      "         -9.4581e-04,  4.6128e-03, -7.7757e-03, -6.1169e-03, -1.0273e-02,\n",
      "          7.2456e-03, -2.6243e-03,  1.3005e-03, -1.3356e-03, -6.4367e-03,\n",
      "          9.0213e-03,  2.6445e-03,  6.0209e-03, -7.9681e-04,  1.7980e-03,\n",
      "          1.0307e-03,  1.0109e-02, -5.6876e-03,  1.4022e-02,  4.2247e-04,\n",
      "         -1.0713e-02,  1.4029e-02, -5.5183e-03,  7.9740e-03, -6.5858e-03],\n",
      "        [ 7.4050e-04, -2.6835e-03,  4.1534e-03,  3.3077e-03, -7.7047e-04,\n",
      "          6.8734e-04, -3.2213e-03,  5.4331e-03,  4.2175e-03,  7.0986e-03,\n",
      "         -5.0232e-03,  1.7955e-03, -9.0893e-04,  9.1763e-04,  4.4321e-03,\n",
      "         -6.2998e-03, -1.8695e-03, -4.1539e-03,  5.8706e-04, -1.2657e-03,\n",
      "         -7.2153e-04, -7.0480e-03,  3.9200e-03, -9.7713e-03, -2.7947e-04,\n",
      "          7.4454e-03, -9.7666e-03,  3.8065e-03, -5.5391e-03,  4.5999e-03],\n",
      "        [-1.9335e-03,  6.3014e-03, -9.7111e-03, -8.1126e-03,  2.0720e-03,\n",
      "         -1.4382e-03,  7.5621e-03, -1.2734e-02, -1.0254e-02, -1.7157e-02,\n",
      "          1.2031e-02, -4.4580e-03,  2.1288e-03, -2.2529e-03, -1.0815e-02,\n",
      "          1.4789e-02,  4.2402e-03,  1.0083e-02, -1.1782e-03,  2.9054e-03,\n",
      "          1.6826e-03,  1.6621e-02, -9.5412e-03,  2.3074e-02,  7.5773e-04,\n",
      "         -1.7714e-02,  2.3125e-02, -9.2440e-03,  1.3196e-02, -1.0793e-02],\n",
      "        [ 2.0545e-03, -6.6310e-03,  1.0215e-02,  8.5720e-03, -2.2071e-03,\n",
      "          1.4955e-03, -7.9573e-03,  1.3398e-02,  1.0826e-02,  1.8104e-02,\n",
      "         -1.2685e-02,  4.7157e-03, -2.2395e-03,  2.3806e-03,  1.1422e-02,\n",
      "         -1.5562e-02, -4.4466e-03, -1.0644e-02,  1.2192e-03, -3.0506e-03,\n",
      "         -1.7693e-03, -1.7497e-02,  1.0074e-02, -2.4294e-02, -8.0765e-04,\n",
      "          1.8664e-02, -2.4354e-02,  9.7585e-03, -1.3905e-02,  1.1357e-02],\n",
      "        [-7.4696e-04,  2.7501e-03, -4.2590e-03, -3.3686e-03,  7.7351e-04,\n",
      "         -7.1513e-04,  3.3013e-03, -5.5692e-03, -4.3006e-03, -7.2447e-03,\n",
      "          5.1333e-03, -1.8252e-03,  9.3181e-04, -9.3438e-04, -4.5170e-03,\n",
      "          6.4562e-03,  1.9250e-03,  4.2367e-03, -6.1387e-04,  1.3012e-03,\n",
      "          7.4014e-04,  7.2184e-03, -3.9966e-03,  1.0006e-02,  2.8019e-04,\n",
      "         -7.6159e-03,  9.9970e-03, -3.8821e-03,  5.6648e-03, -4.7145e-03],\n",
      "        [ 1.5204e-03, -5.6045e-03,  8.6796e-03,  6.8613e-03, -1.5740e-03,\n",
      "          1.4588e-03, -6.7274e-03,  1.1350e-02,  8.7606e-03,  1.4759e-02,\n",
      "         -1.0459e-02,  3.7172e-03, -1.8988e-03,  1.9030e-03,  9.2010e-03,\n",
      "         -1.3157e-02, -3.9240e-03, -8.6305e-03,  1.2526e-03, -2.6523e-03,\n",
      "         -1.5082e-03, -1.4710e-02,  8.1412e-03, -2.0389e-02, -5.6997e-04,\n",
      "          1.5518e-02, -2.0371e-02,  7.9082e-03, -1.1542e-02,  9.6075e-03],\n",
      "        [ 1.5327e-03, -5.7864e-03,  8.9694e-03,  7.0184e-03, -1.5751e-03,\n",
      "          1.5396e-03, -6.9463e-03,  1.1723e-02,  8.9783e-03,  1.5145e-02,\n",
      "         -1.0754e-02,  3.7919e-03, -1.9615e-03,  1.9462e-03,  9.4222e-03,\n",
      "         -1.3585e-02, -4.0799e-03, -8.8482e-03,  1.3315e-03, -2.7511e-03,\n",
      "         -1.5594e-03, -1.5174e-02,  8.3416e-03, -2.1026e-02, -5.6919e-04,\n",
      "          1.5978e-02, -2.0996e-02,  8.1068e-03, -1.1881e-02,  9.9211e-03],\n",
      "        [ 6.5215e-05, -2.7205e-04,  4.2318e-04,  3.1784e-04, -6.4810e-05,\n",
      "          7.8575e-05, -3.2667e-04,  5.5198e-04,  4.0978e-04,  6.9489e-04,\n",
      "         -4.9729e-04,  1.6978e-04, -9.2424e-05,  8.8057e-05,  4.2865e-04,\n",
      "         -6.3884e-04, -1.9709e-04, -4.0445e-04,  6.9655e-05, -1.3169e-04,\n",
      "         -7.3738e-05, -7.1090e-04,  3.8036e-04, -9.8402e-04, -2.3194e-05,\n",
      "          7.4311e-04, -9.8040e-04,  3.7041e-04, -5.5196e-04,  4.6675e-04],\n",
      "        [ 2.1597e-03, -7.0309e-03,  1.0835e-02,  9.0556e-03, -2.3150e-03,\n",
      "          1.6024e-03, -8.4372e-03,  1.4208e-02,  1.1446e-02,  1.9149e-02,\n",
      "         -1.3427e-02,  4.9769e-03, -2.3750e-03,  2.5147e-03,  1.2072e-02,\n",
      "         -1.6501e-02, -4.7290e-03, -1.1254e-02,  1.3119e-03, -3.2410e-03,\n",
      "         -1.8770e-03, -1.8546e-02,  1.0650e-02, -2.5747e-02, -8.4660e-04,\n",
      "          1.9767e-02, -2.5804e-02,  1.0317e-02, -1.4725e-02,  1.2042e-02],\n",
      "        [ 2.1332e-03, -6.9583e-03,  1.0724e-02,  8.9549e-03, -2.2854e-03,\n",
      "          1.5899e-03, -8.3504e-03,  1.4062e-02,  1.1320e-02,  1.8941e-02,\n",
      "         -1.3283e-02,  4.9203e-03, -2.3508e-03,  2.4867e-03,  1.1939e-02,\n",
      "         -1.6331e-02, -4.6837e-03, -1.1131e-02,  1.3029e-03, -3.2090e-03,\n",
      "         -1.8581e-03, -1.8353e-02,  1.0533e-02, -2.5479e-02, -8.3571e-04,\n",
      "          1.9558e-02, -2.5534e-02,  1.0205e-02, -1.4569e-02,  1.1918e-02]])}, 5: {'momentum_buffer': tensor([-0.0179,  0.0125, -0.0294,  0.0310, -0.0128,  0.0260,  0.0268,  0.0013,\n",
      "         0.0329,  0.0325])}, 6: {'momentum_buffer': tensor([[-0.0051, -0.0081,  0.0011,  0.0140,  0.0084,  0.0248,  0.0223, -0.0008,\n",
      "          0.0156,  0.0059],\n",
      "        [ 0.0051,  0.0081, -0.0011, -0.0140, -0.0084, -0.0248, -0.0223,  0.0008,\n",
      "         -0.0156, -0.0059]])}, 7: {'momentum_buffer': tensor([ 0.0470, -0.0470])}}\n",
      "param_groups \t [{'lr': 0.01, 'momentum': 0.1, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "510244fed2737faa98f5c507705b781217d54f2f3afb1f6538ba23bbc323ff21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('nassy-hasler-vBIYXc2u': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}