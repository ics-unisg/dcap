{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "from os import walk\n",
    "from copy import copy\n",
    "import math, pywt, numpy as np\n",
    "#%matplotlib widget\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# data can be found smb://nas-weber01.unisg.ch/data/Nassy/03_Online_Model/data\n",
    "_, _, file_names = next(walk('./split'))\n",
    "\n",
    "files = [(name.split('_')[0], name) for name in file_names]\n",
    "\n",
    "files[0:20]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('005', '005_t1.csv'),\n",
       " ('002', '002_t6.csv'),\n",
       " ('002', '002_t4.csv'),\n",
       " ('002', '002_t2.csv'),\n",
       " ('005', '005_t5.csv'),\n",
       " ('006', '006_t1.csv'),\n",
       " ('001', '001_t4.csv'),\n",
       " ('007', '007_t2.csv'),\n",
       " ('004', '004_t6.csv'),\n",
       " ('006', '006_t4.csv'),\n",
       " ('001', '001_t2.csv'),\n",
       " ('003', '003_t8.csv'),\n",
       " ('002', '002_t1.csv'),\n",
       " ('001', '001_t6.csv'),\n",
       " ('007', '007_t7.csv'),\n",
       " ('005', '005_t6.csv'),\n",
       " ('007', '007_t5.csv'),\n",
       " ('008', '008_t8.csv'),\n",
       " ('003', '003_t1.csv'),\n",
       " ('008', '008_t5.csv')]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tasks = [\n",
    "    ('t1', 't1_gsr', 't1_eclipse (drawing_a_1.java)'),\n",
    "    ('t2', 't2_gsr', 't2_eclipse (drawing_a_2.java)'),\n",
    "    ('t3', 't3_gsr', 't3_eclipse (drawing_b_1.java)'),\n",
    "    ('t4', 't4_gsr', 't4_eclipse (drawing_b_2.java)'),\n",
    "    ('t5', 't5_gsr', 't5_eclipse (drawing_b_3.java)'),\n",
    "    ('t6', 't6_gsr', 't6_eclipse (drawing_b_6.java)'),\n",
    "    ('t7', 't7_gsr', 't7_eclipse (drawing_b_9.java)'),\n",
    "    ('t8', 't8_gsr', 't8_eclipse (drawing_b_11.java)'),\n",
    "]\n",
    "\n",
    "tasks"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('t1', 't1_gsr', 't1_eclipse (drawing_a_1.java)'),\n",
       " ('t2', 't2_gsr', 't2_eclipse (drawing_a_2.java)'),\n",
       " ('t3', 't3_gsr', 't3_eclipse (drawing_b_1.java)'),\n",
       " ('t4', 't4_gsr', 't4_eclipse (drawing_b_2.java)'),\n",
       " ('t5', 't5_gsr', 't5_eclipse (drawing_b_3.java)'),\n",
       " ('t6', 't6_gsr', 't6_eclipse (drawing_b_6.java)'),\n",
       " ('t7', 't7_gsr', 't7_eclipse (drawing_b_9.java)'),\n",
       " ('t8', 't8_gsr', 't8_eclipse (drawing_b_11.java)')]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import json\n",
    "import requests\n",
    "import random, string\n",
    "import pandas as pd\n",
    "import io\n",
    "import time\n",
    "\n",
    "DEFAULT_SETTINGS = {\n",
    "    \"authorisation\": {\n",
    "        'username': 'admin@dcap.local',\n",
    "        'password': 'admin'\n",
    "    },\n",
    "    \"url\": 'http://localhost:8989'\n",
    "}\n",
    "\n",
    "DEFAULT_FILTERS = [\n",
    "    {\n",
    "        'name': \"SubstitutePupilFilter\",\n",
    "        'actualParameters': {\n",
    "            'left_pupil': 'ET_PupilLeft',\n",
    "            'right_pupil': 'ET_PupilRight',\n",
    "            'timestampcolumn': 'Timestamp',\n",
    "        },\n",
    "        'columns': {\n",
    "            'left_pupil': 'ET_PupilLeft',\n",
    "            'right_pupil': 'ET_PupilRight',\n",
    "        }\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'name': \"SubstituteGazePointFilter\",\n",
    "        'actualParameters': {\n",
    "            'leftPupilGazeXName': 'ET_GazeLeftx',\n",
    "            'leftPupilGazeYName': 'ET_GazeLefty',\n",
    "            'rightPupilGazeXName': 'ET_GazeRightx',\n",
    "            'rightPupilGazeYName': 'ET_GazeRighty',\n",
    "            'timestampcolumn': 'Timestamp',\n",
    "        },\n",
    "        'columns': {}\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'name': \"ButterworthFilter\",\n",
    "        'actualParameters': {\n",
    "            'hertz': 4,\n",
    "            'sampleRate': 300,\n",
    "            'timestampcolumn': 'Timestamp',\n",
    "        },\n",
    "        'columns': {\n",
    "            'left_pupil': 'ET_PupilLeft',\n",
    "            'right_pupil': 'ET_PupilRight',\n",
    "        },\n",
    "        'decimalSeparator': '.'\n",
    "    },\n",
    "    {\n",
    "        'name': \"LinearInterpolationFilter\",\n",
    "        'actualParameters': {\n",
    "            'left_pupil': 'ET_PupilLeft',\n",
    "            'right_pupil': 'ET_PupilRight',\n",
    "            'timestampcolumn': 'Timestamp',\n",
    "        },\n",
    "        'columns': {\n",
    "            'left_pupil': 'ET_PupilLeft',\n",
    "            'right_pupil': 'ET_PupilRight',\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "Broken Filter\n",
    "{\n",
    "    'name': \"BlinkDetectionFilter\",\n",
    "    'actualParameters': {\n",
    "        'leftPupilGazeXColumnName': 'ET_GazeLeftx',\n",
    "        'leftPupilGazeYColumnName': 'ET_GazeLefty',\n",
    "        'rightPupilGazeXColumnName': 'ET_GazeRightx',\n",
    "        'rightPupilGazeYColumnName': 'ET_GazeRighty',\n",
    "        'blinkDetectionTimeThreashold': 60,\n",
    "        'left_pupil': 'ET_PupilLeft',\n",
    "        'right_pupil': 'ET_PupilRight',\n",
    "        'timestampcolumn': 'Timestamp',\n",
    "    },\n",
    "    'columns': {}\n",
    "},\n",
    "\"\"\"\n",
    "\n",
    "def randomword(length):\n",
    "   letters = string.ascii_lowercase\n",
    "   return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "def clean(df, settings=DEFAULT_SETTINGS, filters=DEFAULT_FILTERS):\n",
    "    with requests.Session() as s:\n",
    "        # login\n",
    "        r = s.post(\n",
    "            url=settings.get('url') + \"/cheetah/login\", \n",
    "            data=settings.get('authorisation')\n",
    "        )\n",
    "        # create study\n",
    "        try:\n",
    "            data = json.loads(s.post(\n",
    "                url=settings.get('url') + \"/cheetah/api/user/study\",\n",
    "                headers={'content-type': 'application/json'},\n",
    "                data=json.dumps({\n",
    "                    'comment': '',\n",
    "                    'name': randomword(20)\n",
    "                })\n",
    "            ).content.decode('UTF-8'))\n",
    "            study_id = data['resBody'][\"id\"]\n",
    "        except Exception as e:\n",
    "            print('Could not create study')\n",
    "            raise e\n",
    "\n",
    "\n",
    "        # create subject\n",
    "        try:\n",
    "            data = json.loads(s.post(\n",
    "                url=settings.get('url') + \"/cheetah/api/user/addsubject\", \n",
    "                headers={'content-type': 'application/json'},\n",
    "                data=json.dumps({\n",
    "                    'comment': '',\n",
    "                    'email': f'{randomword(20)}@dcap.local',\n",
    "                    'id': 0,\n",
    "                    'studyId': study_id,\n",
    "                    'studyName': randomword(20),\n",
    "                    'subject_id': randomword(20),\n",
    "                    'synchronized_from': 0,\n",
    "                })\n",
    "            ).content.decode('UTF-8'))\n",
    "            subject_id = data['resBody'][\"id\"]\n",
    "        except Exception as e:\n",
    "            print('Could not create subject')\n",
    "            raise e\n",
    "\n",
    "        # upload data\n",
    "        try:\n",
    "            stream = io.StringIO()\n",
    "            df.to_csv(stream, sep='\\t', index=False)\n",
    "\n",
    "            data = json.loads(s.post(\n",
    "                url=settings.get('url') + \"/cheetah/api/user/uploadFile\",\n",
    "                files=[('files', ('file.tsv', stream.getvalue(), 'application/octet-stream'))],\n",
    "                data={\n",
    "                    'subjectIds': [subject_id]\n",
    "                }\n",
    "            ).content.decode('UTF-8'))\n",
    "            file_id = data['resBody']['easyUserDataListSuccessfultMapped'][0][\"id\"]\n",
    "        except Exception as e:\n",
    "            print('Could not upload data')\n",
    "            raise e\n",
    "\n",
    "        # filter data\n",
    "        try:\n",
    "            # request filter\n",
    "            data = json.loads(s.post(\n",
    "                url=settings.get('url') + \"/cheetah/api/user/filterrequest\", \n",
    "                headers={'content-type': 'application/json'},\n",
    "                data=json.dumps({\n",
    "                    \"files\": [file_id],\n",
    "                    \"filters\": filters\n",
    "                })\n",
    "            ).content.decode('UTF-8'))\n",
    "            task_id = data['resBody']\n",
    "\n",
    "            # wait for task to finish\n",
    "            while True:\n",
    "                data = json.loads(s.get(\n",
    "                    url=settings.get('url') + \"/cheetah/api/user/taskFinished/\" + task_id, \n",
    "                    headers={'content-type': 'application/json'},\n",
    "                ).content.decode('UTF-8'))\n",
    "                if data['resBody'] == True:\n",
    "                    break\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            # get filtered file id\n",
    "            data = json.loads(s.get(\n",
    "                url=settings.get('url') + \"/cheetah/api/user/taskid/\" + task_id, \n",
    "            ).content.decode('UTF-8'))\n",
    "            filtered_file_id = data['resBody'][\"id\"]\n",
    "\n",
    "            # download filtered data\n",
    "            data = s.get(\n",
    "                url=settings.get('url') + \"/cheetah/api/user/download/\" + str(filtered_file_id), \n",
    "            ).content.decode(\"utf-8\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Could not filter data')\n",
    "            raise e\n",
    "\n",
    "        # cleanup\n",
    "        # TODO: also delete study and subject\n",
    "        s.delete(\n",
    "            url=settings.get('url') + \"/cheetah/api/user/file/\"+str(file_id)\n",
    "        )\n",
    "        s.delete(\n",
    "            url=settings.get('url') + \"/cheetah/api/user/file/\"+str(filtered_file_id)\n",
    "        )\n",
    "\n",
    "        return pd.read_csv(io.StringIO(data), sep='\\t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "def modmax(d):\n",
    "    m = [0.0]*len(d)\n",
    "    for i in iter(range(len(d))):\n",
    "        m[i] = math.fabs(d[i])\n",
    "\n",
    "        t = [0.0]*len(d)\n",
    "        for i in iter(range(len(d))):\n",
    "            ll = m[i-1] if i >= 1 else m[i]\n",
    "            oo = m[i]\n",
    "            rr = m[i+1] if i < len(d) - 2 else m[i]\n",
    "\n",
    "            if (ll <= oo and oo >= rr) and (ll < oo or oo > rr):\n",
    "                t[i] = math.sqrt(d[i]**2)\n",
    "            else:\n",
    "                t[i] = 0.0\n",
    "\n",
    "    return t\n",
    "\n",
    "def lhipa(d):\n",
    "    w = pywt.Wavelet('sym16')\n",
    "    maxlevel = pywt.dwt_max_level(len(d), filter_len=w.dec_len)\n",
    "\n",
    "    hif, lof = 1, int(maxlevel/2)\n",
    "\n",
    "    if hif <=0 or lof <=0:\n",
    "        return 0.0\n",
    "\n",
    "    cD_H = pywt.downcoef('d', d, 'sym16', 'per', level=hif)\n",
    "    cD_L = pywt.downcoef('d', d, 'sym16', 'per', level=lof)\n",
    "\n",
    "    cD_H[:] = [x/math.sqrt(2**hif) for x in cD_H]\n",
    "    cD_L[:] = [x/math.sqrt(2**lof) for x in cD_L]\n",
    "\n",
    "    cD_LH = cD_L\n",
    "    for i in range(len(cD_L)):\n",
    "        cD_LH[i] = cD_L[i] / cD_H[int(((2**lof)/(2**hif))*i)]\n",
    "\n",
    "    cD_LHm = modmax(cD_LH)\n",
    "\n",
    "    luniv = np.std(cD_LHm) * math.sqrt(2.0*np.log2(len(cD_LHm)))\n",
    "    cD_LHt = pywt.threshold(cD_LHm, luniv, mode=\"less\")\n",
    "\n",
    "    tt = 10#d[-1].timestamp() - d[0].timestamp()\n",
    "\n",
    "    ctr = 0\n",
    "    for i in iter(range(len(cD_LHt))):\n",
    "        if math.fabs(cD_LHt[i]) > 0: ctr += 1\n",
    "\n",
    "    return float(ctr)/tt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def extract_features(df):\n",
    "    df_min, df_max, df_mean, df_median, df_std = df['ET_PubilAvg'].agg([pd.np.min, pd.np.max, pd.np.mean, pd.np.median, pd.np.std])\n",
    "    df['rolling_mean'] = df['ET_PubilAvg'].rolling('8s', min_periods=1).mean()\n",
    "    df_resampled = df[~df.index.duplicated(keep='first')].resample('1ms').pad()\n",
    "    df_resampled['rolling_mean'] = df_resampled['rolling_mean'].shift(periods=-4000)\n",
    "    df_resampled['phasic'] = df_resampled['ET_PubilAvg'] - df_resampled['rolling_mean']\n",
    "    df_resampled['rolling_phasic'] = df_resampled['phasic'].rolling('1s', min_periods=1).mean().shift(periods=-500)\n",
    "    df_resampled['peaks'] = df_resampled['rolling_phasic'][(df_resampled['rolling_phasic'].shift(1) < df_resampled['rolling_phasic']) & (df_resampled['rolling_phasic'].shift(-1) < df_resampled['rolling_phasic']) & (df_resampled['rolling_phasic'] > 0.1)]\n",
    "    peak_count = df_resampled['peaks'].dropna().count()\n",
    "\n",
    "    duration = (df_resampled.index[-1] - df_resampled.index[0]).seconds / 60\n",
    "    peak_count_pm = peak_count / duration\n",
    "\n",
    "    return (df_min, df_max, df_mean, df_median, df_std, peak_count, peak_count_pm, lhipa(df['ET_PubilAvg'].dropna().to_numpy()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "files[52:]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('008', '008_t1.csv'),\n",
       " ('002', '002_t8.csv'),\n",
       " ('003', '003_t3.csv'),\n",
       " ('005', '005_t7.csv'),\n",
       " ('002', '002_t5.csv'),\n",
       " ('008', '008_t7.csv'),\n",
       " ('001', '001_t5.csv'),\n",
       " ('008', '008_t2.csv'),\n",
       " ('005', '005_t3.csv'),\n",
       " ('003', '003_t5.csv'),\n",
       " ('001', '001_t8.csv'),\n",
       " ('003', '003_t2.csv')]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for file_name in files[52:53]:\n",
    "    print(file_name)\n",
    "    df = pd.read_csv(f'./split/{file_name[1]}', comment='#')\n",
    "    #df = df.set_index(pd.TimedeltaIndex(df['timestamp'].values))\n",
    "    df['ET_PupilRight'] = df['ET_PupilRight'].apply(lambda x: x if x > 0 else np.nan)\n",
    "    df['ET_PupilLeft'] = df['ET_PupilLeft'].apply(lambda x: x if x > 0 else np.nan)\n",
    "    df['ET_PubilAvg'] = df.apply(lambda x: (x['ET_PupilLeft'] + x['ET_PupilRight']) / 2, axis=1)\n",
    "    df['Timestamp'] = df['Timestamp'].astype('int64')\n",
    "    df['Timestamp'] = df.apply(lambda x: int(x['Timestamp'] * 1000), axis=1).astype('int64')\n",
    "    baseline = clean(copy(df[df['type'] == 'B']))\n",
    "    baseline = baseline.set_index(pd.to_datetime(baseline['Timestamp'].values))\n",
    "    b_min, b_max, b_mean, b_median, b_std, b_peak_count, b_peak_count_pm, b_lhipa = extract_features(baseline)\n",
    "\n",
    "    experiment = clean(copy(df[df['type'] == 'M']))\n",
    "    experiment = experiment.set_index(pd.to_datetime(experiment['Timestamp'].values))\n",
    "    e_min, e_max, e_mean, e_median, e_std, e_peak_count, e_peak_count_pm, e_lhipa = extract_features(experiment)\n",
    "\n",
    "    d_mean = b_mean - e_mean\n",
    "    d_median = b_median - e_median\n",
    "    d_std = b_std - e_std\n",
    "\n",
    "    f = open(f'dataset/{file_name[1]}', \"a\")\n",
    "    f.write(f'task,subject,d_mean,d_median,d_std,e_min,e_max,e_mean,e_median,e_std,e_peak_count,e_peak_count_pm,b_min,b_max,b_mean,b_median,b_std,b_peak_count,b_peak_count_pm,e_lhipa,b_lhipa\\n')\n",
    "    f.write(f'{file_name[1].split(\"_\")[1].split(\".\")[0]},{file_name[0]},{d_mean},{d_median},{d_std},{e_min},{e_max},{e_mean},{e_median},{e_std},{e_peak_count},{e_peak_count_pm},{b_min},{b_max},{b_mean},{b_median},{b_std},{b_peak_count},{b_peak_count_pm},{e_lhipa},{b_lhipa}')\n",
    "    f.close()\n",
    "\n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('008', '008_t1.csv')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/martin/.virtualenvs/nassy-hasler-vBIYXc2u/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (3,4,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity = \"none\" if silent else self.ast_node_interactivity\n",
      "<ipython-input-6-f4d72e74e83b>:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  df_min, df_max, df_mean, df_median, df_std = df['ET_PubilAvg'].agg([pd.np.min, pd.np.max, pd.np.mean, pd.np.median, pd.np.std])\n",
      "<ipython-input-6-f4d72e74e83b>:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  df_min, df_max, df_mean, df_median, df_std = df['ET_PubilAvg'].agg([pd.np.min, pd.np.max, pd.np.mean, pd.np.median, pd.np.std])\n",
      "<ipython-input-6-f4d72e74e83b>:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  peak_count_pm = peak_count / duration\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "510244fed2737faa98f5c507705b781217d54f2f3afb1f6538ba23bbc323ff21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('nassy-hasler-vBIYXc2u': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}